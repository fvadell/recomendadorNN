{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hecho \\n    por\\n      fvadell\\n          ^•ﻌ•^ฅ♡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Al final terminé usando sólo los leads pero bueno...\n",
    "hits_file = '../../Data/hits_ZPAR.csv'\n",
    "hits = pd.read_csv(hits_file)\n",
    "leads_file = '../../Data/leads_ZPAR.csv'\n",
    "leads = pd.read_csv(leads_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las columnas que debería tener el dataset son estas:\n",
    "#['idaviso', 'ciudad', 'provincia', 'precio', 'tipodeoperacion',\n",
    "#'tipodepropiedad', 'habitaciones', 'metrostotales', 'iscurrent',\n",
    "#'idusuario', 'lead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.shape, leads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['lead'] = 0\n",
    "leads['lead'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads = leads[hits.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_leads = pd.concat([hits, leads], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_leads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si hay combinaciones de usuario - aviso repetidas me quedo con los leads unicamente\n",
    "hits_leads = hits_leads.sort_values('lead', ascending = False).drop_duplicates(['idusuario', 'idaviso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me quedo con los usuarios que tienen entre 5 y 50 leads\n",
    "v = leads.idusuario.value_counts()\n",
    "leads = leads[leads.idusuario.isin(v.index[(v.gt(5))&(v.lt(50))])]\n",
    "leads.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo los leads falsos y los marco con la variable 'Match'\n",
    "false_leads = leads.copy()\n",
    "false_leads['idusuario'] = false_leads['idusuario'].sample(frac=1).values\n",
    "leads['Match'] = 1\n",
    "false_leads['Match'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads.shape, false_leads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads = pd.concat([leads, false_leads])\n",
    "del false_leads\n",
    "leads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora el dataframe de leads tiene leads verdaderos y leads falsos.\n",
    "# El modelo va a tratar de predecir cuál es cuál, es decir 'Match'\n",
    "userid = leads.idusuario.sample(1).item()\n",
    "leads[leads['idusuario']==userid].sort_values('Match', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- Caracteristicas de dataset --\\n')\n",
    "print('Total de filas: {}'.format(leads.shape[0]))\n",
    "print('La mitad de las filas son leads falsos (la variable Match está en 0)')\n",
    "print('Total de usuarios: {}'.format(leads.idusuario.nunique()))\n",
    "print('En promedio cada usuario aparece {:.2f} veces'.format(leads.shape[0]/leads.idusuario.nunique()))\n",
    "print('Total de avisos: {}'.format(leads.idaviso.nunique()))\n",
    "print('En promedio cada aviso aparece {:.2f} veces'.format(leads.shape[0]/leads.idaviso.nunique()))\n",
    "print('Total de ciudades: {}'.format(leads.ciudad.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = leads.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "  def __init__(self, emb_dims, no_of_cont, lin_layer_sizes,\n",
    "               output_size, emb_dropout, lin_layer_dropouts):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    emb_dims: Lista de tuplas. \n",
    "        Hay una tupla por cada variable\n",
    "    categorica. La tupla contiene la cantidad de valores posibles\n",
    "    de la variable y la dimension del embedding.\n",
    "\n",
    "    no_of_cont: Integer\n",
    "        Cantidad de variables numericas.\n",
    "\n",
    "    lin_layer_sizes: Lista de enteros.\n",
    "        Una lista con el  tamaño de cada capa lineal.\n",
    "\n",
    "    output_size: Integer\n",
    "        El tamaño de la capa de salida.\n",
    "\n",
    "    emb_dropout: Float\n",
    "        Dropout luego de cada capa de embeddings.\n",
    "\n",
    "    lin_layer_dropouts: Lista de floats\n",
    "        Dropout luego de cada capa lineal.\n",
    "    \"\"\"\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    # Embedding layers\n",
    "    self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n",
    "                                     for x, y in emb_dims])\n",
    "\n",
    "    no_of_embs = sum([y for x, y in emb_dims])\n",
    "    self.no_of_embs = no_of_embs\n",
    "    self.no_of_cont = no_of_cont\n",
    "\n",
    "    # Linear Layers\n",
    "    first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont,\n",
    "                                lin_layer_sizes[0])\n",
    "\n",
    "    self.lin_layers =\\\n",
    "     nn.ModuleList([first_lin_layer] +\\\n",
    "          [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n",
    "           for i in range(len(lin_layer_sizes) - 1)])\n",
    "    \n",
    "    for lin_layer in self.lin_layers:\n",
    "      nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "    # Output Layer\n",
    "    self.output_layer = nn.Linear(lin_layer_sizes[-1],\n",
    "                                  output_size)\n",
    "    nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "\n",
    "    # Batch Norm Layers\n",
    "    self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n",
    "    self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size)\n",
    "                                    for size in lin_layer_sizes])\n",
    "\n",
    "    # Dropout Layers\n",
    "    self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "    self.droput_layers = nn.ModuleList([nn.Dropout(size)\n",
    "                                  for size in lin_layer_dropouts])\n",
    "\n",
    "  def forward(self, cont_data, cat_data):\n",
    "\n",
    "    if self.no_of_embs != 0:\n",
    "      x = [emb_layer(cat_data[:, i])\n",
    "           for i,emb_layer in enumerate(self.emb_layers)]\n",
    "      x = torch.cat(x, 1)\n",
    "      x = self.emb_dropout_layer(x)\n",
    "\n",
    "    if self.no_of_cont != 0:\n",
    "      normalized_cont_data = self.first_bn_layer(cont_data)\n",
    "\n",
    "      if self.no_of_embs != 0:\n",
    "        x = torch.cat([x, normalized_cont_data], 1) \n",
    "      else:\n",
    "        x = normalized_cont_data\n",
    "\n",
    "    for lin_layer, dropout_layer, bn_layer in\\\n",
    "        zip(self.lin_layers, self.droput_layers, self.bn_layers):\n",
    "        x = F.relu(lin_layer(x))\n",
    "        x = bn_layer(x)\n",
    "        x = dropout_layer(x)\n",
    "\n",
    "    x = self.output_layer(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"idusuario\", \"idaviso\", \"ciudad\", \"tipodeoperacion\"]\n",
    "output_feature = \"Match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizo el precio\n",
    "# Y trunco los valores por encima del percentil 90. Esto quizas es demasiado.\n",
    "# TODO: Habría que normalizarlo por cada tipo de operacion para que esté bien.\n",
    "data = data[categorical_features + ['precio'] + ['Match']]\n",
    "m = data.precio.quantile(.90)\n",
    "data['precio'] = data.precio.apply(lambda x: min(m, x))\n",
    "data['precio'] = (data['precio']-data['precio'].mean())/data['precio'].std()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.precio.plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encodding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Le aplico label encodding a cada variable categorica y me guardo los diccionarios en la lista mappings.\n",
    "mappings = []\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    data[col] = data[col].astype('category')\n",
    "    mappings.append(dict(zip(le.classes_, range(len(le.classes_)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo el Dataset de Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "  def __init__(self, data, cat_cols=None, output_col=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Characterizes a Dataset for PyTorch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    data: pandas data frame\n",
    "      The data frame object for the input data. It must\n",
    "      contain all the continuous, categorical and the\n",
    "      output columns to be used.\n",
    "\n",
    "    cat_cols: List of strings\n",
    "      The names of the categorical columns in the data.\n",
    "      These columns will be passed through the embedding\n",
    "      layers in the model. These columns must be\n",
    "      label encoded beforehand. \n",
    "\n",
    "    output_col: string\n",
    "      The name of the output variable column in the data\n",
    "      provided.\n",
    "    \"\"\"\n",
    "\n",
    "    self.n = data.shape[0]\n",
    "\n",
    "    if output_col:\n",
    "      self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n",
    "    else:\n",
    "      self.y =  np.zeros((self.n, 1))\n",
    "    self.y = torch.Tensor(self.y).to(device)\n",
    "\n",
    "    self.cat_cols = cat_cols if cat_cols else []\n",
    "    self.cont_cols = [col for col in data.columns\n",
    "                      if col not in self.cat_cols + [output_col]]\n",
    "\n",
    "    if self.cont_cols:\n",
    "      self.cont_X = data[self.cont_cols].astype(np.float32).values\n",
    "    else:\n",
    "      self.cont_X = np.zeros((self.n, 1))\n",
    "    self.cont_X = torch.Tensor(self.cont_X).to(device)\n",
    "\n",
    "    if self.cat_cols:\n",
    "      self.cat_X = data[cat_cols].astype(np.int64).values\n",
    "    else:\n",
    "      self.cat_X =  np.zeros((self.n, 1))\n",
    "    self.cat_X = torch.Tensor(self.cat_X).to(device).int()\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.n\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    One sample of data.\n",
    "    \"\"\"\n",
    "    return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso GPU si se puede\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset(data=data, cat_cols=categorical_features,output_col=output_feature, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 2**8\n",
    "dataloader = DataLoader(dataset, batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaño de los embeddings con maximo en 100.\n",
    "# La formulita de (x+1)//2 la saque de algun lado\n",
    "cat_dims = [int(data[col].nunique()) for col in categorical_features]\n",
    "emb_dims = [(x, min(100, (x + 1) // 2)) for x in cat_dims]\n",
    "emb_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, no_of_cont=1, lin_layer_sizes=[50, 100],\n",
    "                          output_size=1, emb_dropout=0.04,\n",
    "                          lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=20, lr = 0.01):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        for y, cont_x, cat_x in dataloader:\n",
    "            \n",
    "            # Forward Pass\n",
    "            preds = model(cont_x, cat_x)\n",
    "            loss = criterion(preds, y)\n",
    "\n",
    "            # Backward Pass and Optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch {} - Loss {:.5f}'.format(epoch, loss), end = '\\r')\n",
    "    print('Entrené en {} Epochs.  ฅ^•ﻌ•^ฅ OK!'.format(epochs), end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerar que no estoy usando set de validación. Habría que hacerlo idealmente para meter un earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train(model, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veo la accuracy en un batch\n",
    "y,cont,cat = next(iter(dataloader))\n",
    "(model(cont, cat).round()==y).sum()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features,cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_position(userid):\n",
    "    return mappings[0][userid]\n",
    "\n",
    "def get_ciudad_position(ciudad):\n",
    "    return mappings[2][ciudad]\n",
    "\n",
    "pos_to_ciudad = dict(zip(mappings[2].values(), mappings[2].keys()))\n",
    "pos_to_user_id = dict(zip(mappings[0].values(), mappings[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso annoy para conseguir las ciudades más similares. También se podría hacer para los embeddings de usuarios y de items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "f = 100  # len de los vectores\n",
    "\n",
    "c = AnnoyIndex(f, 'euclidean')\n",
    "n_embeddings = model.emb_layers.state_dict()['2.weight'].shape[0] # Busco los embeddings guardados en el modelo\n",
    "\n",
    "for i, emb in enumerate(model.emb_layers.state_dict()['2.weight']):\n",
    "    c.add_item(i, emb)\n",
    "    print('Progress: {}/{}'.format(i,n_embeddings), end = '\\r')\n",
    "print('Progress: {}/{} ... ฅ^•ﻌ•^ฅ OK!'.format(n_embeddings, n_embeddings), end = '\\r')\n",
    "\n",
    "c.build(40) # Número de árboles de ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciudades_similares(ciudad, cantidad=4):\n",
    "    pos = get_ciudad_position(ciudad)\n",
    "    similar_ciudades = c.get_nns_by_item(pos,cantidad+1)\n",
    "    similar_ciudades = [pos_to_ciudad[ciudad] for ciudad in similar_ciudades]\n",
    "    similar_ciudades.remove(ciudad)\n",
    "    return similar_ciudades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ciudades_similares('Núñez')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecomendadorNN",
   "language": "python",
   "name": "recomendadornn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
